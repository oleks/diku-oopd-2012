\chapter{Meh}

This primal chapter introduces the most fundamental notions of imperative
programming. Many concepts that you already (should) know from Functional
Programming you'll find reiterated throughout this chapter. These reiterations
are made not to irritate you, but to allow you to naturally build up a
framework for distinction between the imperative and functional paradigms.

\begin{definition}

\kw{Computers} are entities with \kw{state} and \kw{computational units},
capable of performing \kw{actions} that change the state of the computer.

\end{definition}

While it may be modern (about time) to speak of machines as having multiple
computational units, call them \kw{cores}, for a brief moment it is best for
our discussion to restrain ourselves to single-core machines.

\begin{definition}

``An action is a happening, taking place in a finite period of time and
establishing a well-defined, intended \kw{net-effect}.''
\cite{dijkstra-introduction}

\end{definition}

This definition highlights two important points:

\begin{enumerate}

\item An action occurs over a finite period of time. This allows us to speak of
a point in time $T_n$ and $T_{n+1}$, as the time of action inception and
termination, respectively.

\item An action has a well-defined, intended net-effect. This is to emphasize
our interest in the outcome of an action rather than the action itself.

\end{enumerate}

To this end, it makes sense to describe the ``net-effect'' of an action as the
difference between the state of the machine at time $T_n$ and $T_{n+1}$. This
notion breaks down as we turn to multi-core machines where the clear benefit of
performing multiple actions simultaneously is utilized. Therefore, apply this
notion with care.

The reason this benefit is ``clear'' comes up when we discuss the notion of
state itself. We could define the state of a computer as a sequence $S$, of
equally-sized chunks, called \kw{words}. The computational unit is historically
only capable of performing actions on a fixed number of such words at a time.
Usually, this number is much smaller than $S$. As a matter of fact, it is often
theoretically beneficial to let the size of $S$ be infinite.

You can imagine the computational unit in a single-core computer as a lonely
zero-dimensional librarian in a large one-dimensional library
(state)\footnote{Both exist in discrete space. Clearly, the librarian is a dot,
while the library can be a line, segment, or ray, all depending on the
theoretical applicability of either.}. The library consists of equally-sized
tapes (words) arranged in a linear fashion.  As tapes do not overlap, the
consequent location of the tapes superimposes an \kw{address} for every tape.

The librarian is always at some particular address in the library, and can only
work with the tape at that particular location. The librarian is unable to work
on multiple tapes simultaneously, since the librarian moves through space, one
location at a time, to the other tape of interest as a part of that action.
Yet, it is practical for the librarian to be able to work on multiple such
tapes at a time. For instance, the librarian may wish to merge two tapes into
one.

%\marginpar{``электронная машина и решения принимала бы быстрее, и просчитывала
%намного больше вариантов, и не спешила бы домой в конце рабочего дня, и не
%делала бы ничего «по блату»''}


Therefore, if we had to describe what constitutes an action, we'd say that in
some time frame $T_1$, the librarian should be able to \begin{inparaenum}[(1)]
\item grab a few tapes, \item examine the data on these tapes, \item overwrite
them with new data, if necessary, and \item put the tapes back in their
original locations\end{inparaenum}.

It is important to reiterate that this sequence of the sub-actions,
constituting an action, has to happen in the fixed time frame of $T_1$, in
particular, such that $T_n+T_1=T_{n+1}$.  This means that regardless of the
location of the tapes required for some action, the action must take at most
time $T_1$. Yet, the librarian has to move about in order to pick up and put
down various tapes. In the worst case, the tapes of interest may be at opposite
ends of the library. Therefore, the larger the library, the larger the $T_1$
has to be. 

Instead of letting our concept of time vary on the size of our library, we
usually let our state $S$ possess a property known as \kw{random access}. In a
way, this is akin to giving our librarian teleporting abilities. This allows
the concept of time to be constrained to \begin{inparaenum}[(1)] \item the
number of tapes a librarian is interested in, \item the time it takes to
examine and overwrite these tapes, as well as \item perform various
computations on the examined tapes\end{inparaenum}. However, we need not dig
deeper down this rabbit hole here.

\begin{definition}

A state $S$ is a sequence of equally sized chunks with random access.

\end{definition}

The librarian analogy is pretty lousy, as a librarian that overwrites the
library tapes, is surely to be fired rather promptly. The key here is to have
more tapes than actual data. Computers, unlike librarians are interested in
doing more with their data than just move it about.

Peter naur.

This is because computer science is both about
managing data and producing it. 

This sequence of steps constitutes and action. Note, that there is no movement
of tapes, just reading and writing, everything else, is done by a combination
of such actions.


It gets to be a pretty tiresome work when the size of
$S$ is large, or even worse, infinite. It is therefore clear that multiple computational
units could do things faster. The real trouble comes along when these
computational units have to use the same tape, but that is outside the scope of
this course.

In particular, multi-core machines usually share the same state.


% action -> action clusters (Naur)

\section{State of the art}

Traditionally modules. Literate Programming. C\# has regions.

\section{Exercises}
