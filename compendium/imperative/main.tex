\chapter{Assemblage}

% to be introduced in one chapter:
%
% (declarative), imperative, procedural, and structured programming.
%
% under procedural: subroutines vs. coroutines.
%
% primitive types, arrays, structures, value and reference types.
%
% next chapter: data structures.
%
% material for first week: the first two chapters.

% humans can only remember about 4 things at once -- the registers of the
% librarian -- see short-term memory article on wikipedia.


This chapter introduces some of the most fundamental notions of imperative and
procedural programming.  Many concepts that you already (should) know from your
experience with funcational programming, you'll find reiterated throughout this
chapter.  This reiteration, I hope, provides a yardstick for distinguishing
between declarative, imperative and procedural programming.

\vspace{0.5in}

What you've been practising thus far, is mostly the \key{declarative}
programming paradigm.

\begin{definition}

In a purely declarative programming paradigm, values are \key{immutable}, and
programming is done by describing the relationships between values using
\key{functions}.

\end{definition}

\marginpar{Exercise: Write an if-then-else-if-else-statement function in SML.
(requires explanation of else-if)}

Less formally, in pure declarative programming, we avoid describing the
\key{flow} of the program and resort to pure function application, or rather,
function application is the only program flow connective we've got.  In this
paradigm, the programmer is concerned with the mapping of input values to
output values, such that the mapping defines a \key{class of computations} that
the program can evoke.  The programmer is \emph{not} concerned with being
particularly clear about the flow of data or control throughout the program, as
this is implicit in the mapping.

\marginpar{A course in Object-oriented programming has historically always
seemed easier on students after a course on Functional Programming.}

As you may have already noticed, this paradigm can be notoriously difficult to
apply to certain problems, despite (or due to) it's neat mathematical
properties.  What's more, the more accurate student would've already noticed
that you've not done \emph{pure} declarative programming, but rather, already
have experience with a few \key{control flow structures} such as \key{if-} and
\key{case-} \key{statements}. Why this is, will become apparent shortly. We
will return to these, and many more control flow structures, in a bit as well.

An alternative programming paradigm, the \key{imperative}, is historically
closer to hardware. It emerged as the programming paradigm for computer
architectures deriving from John von Neumann's original digital computer
architecture proposal \cite{von-neumann}, henceforth referred to as the von
Nuemann architecture. By this architecture, computers are rather feebleminded.
Inevitebly, the programmer has to be the clever one when specifying the program
flow.

As a by-product, the programmer can often get carried away with some
non-essential details, rather than concentrate on the problem at hand. Indeed,
being ``so close to the bear metal'', the paradigm opens up for marvelous
time-spenders on opposing programming frontiers. On the one hand, the
programmer may have to write a great deal of code to even get the simplest
things done, while on the other hand, the programmer can spend a great deal of
time looking for clever tricks to make some part of the program marginally
faster on some particular architecture.

Blah blah conventional CS problem -- abstraction vs. performance.

Time has shown, that neither is a dominating strategy practise. Any
self-respecting programmer knows that premature optimization is the root of all
evil \cite{knuth-goto}, as any self-respecting manager knows not to measure
programmer productivity by the number of lines written.

\marginpar{Exercise: A good computer scientist would quickly come up with a
scheme for generating good for nothing intermediate code, and get paid for not
doing much work at all.}

The more accurate productivity measure seems to be an evaluation of
\begin{inparaenum}[(1)] \item the class of computations that the program can
evoke, and \item the \key{time-} and \key{resource} \key{requirements} of such
evocations\end{inparaenum}. Hence, the programmer's job becomes the more poetic
-- ``designing a class of computations'', rather than the more mundane --
''writing a program''. In essence, it seems more fruitful for us to be more
concerned with the behaviour of programs, rather than their contruction.

This view may be acceptable so long as we're not reminded that programs often
have to evolve, either due to changes in the program requirements, or due to
bugs in the original implementation. What's more, programs are usually evolved
by programmers other than their original programmers. In order to evolve a
program \emph{efficiently}, the epoch programmers have to be able to put
themselves in the shoes of the original programmers, and understand why the
program was constructed the way that it was.  This allows them to evolve the
program efficiently, rather than having to resort to say, rewriting the program
from scratch\footnote{Sometimes, the only way to efficiently evolve a program
is to write it from scratch.}.

\begin{definition}

Efficient program evolution is an evolution that increases program compliance
with its requirements, while introducing the least possible changes to the
original program, and accomodating possible future program  evolution.

\end{definition}

In essence, as programmers, we're playwriters for two rather distinct
audiences: the feebleminded computer that has to evoke the program, and the
generally clever, but sometimes feebleminded programmers that have to evolve
the program. In order to contrast these two audiences, we first need an
abstract understanding of the infamous von Neumann architecture.

\begin{definition}

\key{Computers} are entities with \key{state} and \key{computational units}.
Computational units perform \key{actions} that change the state of the
computer.

\end{definition}

While it may be modern (about time) to speak of machines as having multiple
computational units, call them \key{cores}, for a brief moment it is best for
our discussion to restrain ourselves to single-core machines.

\begin{definition}

``An action is a happening, taking place in a finite period of time and
establishing a well-defined, intended \key{net-effect}.''
\cite{dijkstra-introduction}

\end{definition}

This definition highlights two important points:

\begin{enumerate}

\item An action occurs over a finite period of time. This allows us to speak of
a point in time $T_n$ and $T_{n+1}$, as the time of action inception and
termination, respectively.

\item An action has a well-defined, intended net-effect. This is to emphasize
our interest in the outcomes of actions rather than the actions themselves.

\end{enumerate}

To this end, it makes sense to describe the ``net-effect'' of an action as the
difference between the state of the machine at time $T_n$ and $T_{n+1}$. This
notion breaks down as we turn to multi-core machines where the clear benefit of
performing multiple actions at once has been utilized. Therefore, apply this
notion with care.

The reason this benefit is ``clear'' comes up when we discuss the notion of
state. We can define the state of a computer as a sequence $S$, of
equally-sized chunks, called \key{words}. The computational unit is
historically only capable of performing actions on a fixed number of such words
at a time.  Usually, this number is much smaller than the length of the
sequence $S$. As a matter of fact, it is often theoretically beneficial to let
$S$ be infinite in size.

You can imagine the computational unit in a single-core computer as a lonely
librarian in a large one-dimensional tape library\footnote{Both exist in
discrete space. Clearly, the librarian is a dot, while the library can be a
line, segment, or ray, all depending on the theoretical applicability of
either.}. To comply with our present notion of state, let the library consist
of equally-sized, non-overlapping tapes arranged in a linear fashion. The
consequent location of the tapes superimposes an \key{address} for every tape.
Even if the library is of an infinite size, an address can be superimposed
relative to the position of the librarian. In a sense, this is akin to house
numbering along a street.

The librarian is always at some particular location in the library, and can
only work on the tape at that location. Initially, the librarian is unable to
work on multiple randomly distributed tapes simultaneously, since the librarian
has to be ``on site'' to work on particular tapes. If in the course of an
action, the librarian wishes to work on some fixed number of disjoint tapes,
the librarian has to move through the library, from one location to the other,
one location at a time -- as a part of that action. It is practical for the
librarian to be able to do this. For instance, the librarian may wish to merge
two tapes into one.

\marginpar{

``\cyrtext{электронная машина и решения принимала бы быстрее, и просчитывала
намного больше вариантов, и не спешила бы домой в конце рабочего дня}''

Translation: The electronic machine would've made decisions faster, considered
more options, and wouldn't hurry home at the end of the day.

}

Therefore, if we had to describe what constitutes an action, we'd say that in
some time frame $T_1$, the librarian should be able to \begin{inparaenum}[(1)]
\item pick up a fixed number of spatially disjoint tapes, \item perform
operations on those tapes \item put the tapes back on their original
locations\end{inparaenum}.

It is important to reiterate that this sequence of the sub-actions,
constituting an action, has to happen in the fixed time frame of $T_1$, in
particular, such that $T_n+T_1=T_{n+1}$.  This means that regardless of the
location of the tapes required for some action, the action must take at most
time $T_1$. Yet, the librarian has to move about in order to pick up and put
down various tapes. In the worst case, the tapes of interest may be at opposite
ends of the (finite) library. Therefore, the larger the library, the larger the
$T_1$ has to be. 

Instead of letting our concept of time vary on the size of our library, we
usually make an abstraction and let our state $S$ possess a property known as
\key{random access}. In a way, this is akin to giving our librarian teleporting
abilities, but really we're just letting $T_1$ be the time it. This allows the
concept of time to be constrained to \begin{inparaenum}[(1)] \item the number
of tapes a librarian is interested in for any action, \item the time it takes
to examine and overwrite these tapes, as well as \item perform various
computations on the examined tapes\end{inparaenum}. However, we need not dig
deeper down this rabbit hole here.

\begin{definition}

A state $S$ is a sequence of equally sized chunks with random access.

\end{definition}

The librarian analogy is pretty lousy, as a librarian that overwrites the
library tapes, is surely to be fired rather promptly. The solution to this
analogy dilemma is to have a range of empty tapes that serve as auxiliaries for
the librarian's various operations, and to assume that tape overwrites are
lossless. 

\marginpar{Exercise: XOR swap}

In general, we'd like for our computers, unlike our librarians, to do more than
simply store our data in a neat order. We want for our computers to sort, move
about, generate and actually overwrite data on demand. These, and many more
data operations facilitate our actual goal -- the solution of various
real-world problems.

Peter naur.

This is because computer science is both about
managing data and producing it. 

This sequence of steps constitutes and action. Note, that there is no movement
of tapes, just reading and writing, everything else, is done by a combination
of such actions.


It gets to be a pretty tiresome work when the size of
$S$ is large, or even worse, infinite. It is therefore clear that multiple computational
units could do things faster. The real trouble comes along when these
computational units have to use the same tape, but that is outside the scope of
this course.

In particular, multi-core machines usually share the same state.


% action -> action clusters (Naur)


\begin{quote}"You have perhaps had a dream much like mine: Wouldn't it be nice
to have a glorious system of complete maps of the world, whereby one could (by
turning dials) increase or decrease the scale at will? A similar thing can be
achieved for programs, rather easily, when we give the programs a hierarchic
structure like those constructed step-wise. It is not hard to imagine a
computing system which displays a program in such a way that, by pressing
an-appropriate button, one can replace the name of a routine by its expansion
on the next level, or conversely."\cite{knuth-review-of-sp}\end{quote}

\emph{Fun fact:} (should come after discussion of representation of data types
with multiple words) The rooms at DIKU have a peculiar numbering system that
has a lot to do with the way addressing happens in a computer. If you've ever
seen DIKU from the outside, you might've noticed that, by design, all the
windows of a building are of equal size and the windows are equally spaced,
creating a rather aesthetic outside look. The ``address'' of a room inside DIKU
consists of 3 parts, building, floor and window. Buildings have number codes,
as do floors. This leads you to the correct hall, as on any floor there is only
one hallway. The last part of the address indicates the window at which the
room begins, numbered from east to west, (0-indexed). The analogy breaks down
when we find out that not all rooms in a given hall are of the same size.



\section{State of the art}

Traditionally modules. Literate Programming. C\# has regions.

\input{imperative/exercises}
