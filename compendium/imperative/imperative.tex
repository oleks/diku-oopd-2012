An alternative programming paradigm, the \key{imperative}, is historically
closer to hardware. It emerged as the programming paradigm for computer
architectures stemming from John von Neumann's original proposal for the design
of the digital computer\cite{von-neumann}.  A key aspect of the von Neumann
architecture is that it keeps the computer rather feebleminded, leaving it to
the programmer to be the clever one. Inevitably, no good introduction to the
imperative programming paradigm can commence, without at least a gentle
introduction to the von Neumann architecture.

\begin{definition}

A \key{computer}\footnotemark is an entity with \key{state} and
\key{processors}. A processor performs \key{actions} that change the state of
the computer.

\footnotetext{Although at the beginning of the 20th century, a ``computer'' was
still a profession, today most computers are electronic machines.  We'll use
the words ``computer'' and ``machine'' interchangably.}

\end{definition}

While it may be modern (about time) to speak of computers as machines with
multiple processors, we will restrain ourselves to single processor machines.
Programming for multiple processors raises a range of problematics irrelevant
to the core matter of these lecture notes. At times it will provide for an
interesting discussion to consider how certain aspects map over into
multiprocessor machines, but in such cases they will be mentioned explicitly.

\begin{definition}

``An action is a happening, taking place in a finite period of time and
establishing a well-defined, intended \key{net effect}.''
\cite{dijkstra-introduction}

\end{definition}

This definition highlights two important points. Firstly, an action takes place
in a finite period of time, say $T_1$. This allows us to speak of the points in
time $T_n$ and $T_{n+1}$, as the times of action inception and termination,
i.e. $T_{n+1}=T_n+T_1$. Secondly, an action establishes a well-defined,
intended net effect. This highlights that we are interested in actions that do
something, indeed something expected.

To this end, it makes sense to describe the ``net effect'' of an action as the
difference between the state of the computer at time $T_n$ and $T_{n+1}$.
However, this notion breaks down as we turn to multiprocessor machines, where
the clear benefit of performing multiple actions at once has been utilized.
Apply this notion with care.

% ``action â€“ a function or a function object that mutates the value of an
% object'' -- Alexander Stepanov, Adobe notes.

As an example of an action, consider doing the laundry. Given a pile of dirty
laundry, the net effect of this action is the same pile of laundry, but clean.
Clearly, the functional paradigm fits abnormally to this type of problem.
Knitting a pile of clean laundry, same but different, is not nearly as
practical as cleaning the dirty pile. Indeed, why should we dispose of a pile
of perfectly good clothing, just because it has gotten a little dirty? The
process of cleaning, rather than knitting, is explicit to the programmer in the
imperative paradigm.

While modifying global state is a marvel of imperative programming, it is
important that we achieve an intended net effect; nothing more, nothing less.
So while we wish to change the state of the world in some particular way, we
also wish to avoid unintended \key{side-effects}.  This is to avoid a general
``butterfly effect'', where doing the laundry causes a thunderstorm on the
other side of the globe.

% A bit too early for this, but worth noting in general: ``The great discovery
% of Turing and von Neumann that set us on a new path was the discovery of
% memory.  We are not just dealing with numbers: we are storing them in
% different locations.''\cite{stepanov-adobe}}

% A programming language facilitates a programming paradigm if it is both easy,
% and reasonably safe to use the paradigm.

One technique that the imperative paradigm shares with the functional is
\key{data abstraction}. The idea is to group the elements of the world into
well-defined \key{data structures} and modify data structures rather than
arbitrary global state.

\begin{definition}

A data structure is a particular ordering of data elements. A data structure
has well-defined methods of modification that maintain this ordering.

\end{definition}

The ``data elements'' that a data structure maintains are data structures
themselves. So the methods of modification of a particular data structure have
to take into account the methods of modification of its constituent data
elements.

This technique allows for arbitrary data abstraction, but of course, at some
point, some actual modification of the global state has to commence. A computer
therefore has  at least one \key{basic} data structure, whos methods of
modification are implemented in hardware. We'll refer to such a basic method of
modification as an \key{instruction}. The state of the computer can then be a
finite \key{array} of instances of this basic data structure. We call this
array the computer's \key{memory}.

\begin{definition}

An array is a collection of data elements. Every element has a unique
identifier, called an \key{index}, which we can use to \key{access} the
element. We access an element whenever we need to analyze, or modify its value.
Accessing any element in the array roughly the same amount of time, this
property is called \key{random access}.

\end{definition}

An array is a data structure. For all intents and purposes it does not matter
\emph{how} the elements are actually ordered. We would simply like to be able
to \key{access} the elements in some arbitrary order.

One frequently used basic data structure is a \key{word}: a fixed-size group of
bits, typically 4 bytes in size. For example:

\begin{equation}
0000 0000\ 0000 0000\ 0000 0000\ 0000 0000 \label{word-0}
\end{equation}

A possible basic modification method is to flip all the bits of a word, known
as binary negation. We'll refer to this operation as $\proc{Neg}$. With
$\proc{Neg}$ we can go back and forth between \ref{word-0} and \ref{word-1}:

\begin{equation}
1111 1111\ 1111 1111\ 1111 1111\ 1111 1111 \label{word-1}
\end{equation}

Such a data structure may not seem particularly useful, but it \emph{can}
represent a boolean value. An $n$-size array of such values could for example
be used to automatically monitor a parking lot with $n$ parking spots, where
each spot is either ``free'' or ``occupied''. Imagine that we have an auxiliary
sensor system that notifies us whenever a car enters or leaves a particular
spot. We can then $\proc{Neg}$ a corresponding word whenever either event
occurs. However, for such a system to operate consistently this instruction is
not enough.

Initially, the parking lot is completely empty. Regardless of whether we chose
to use \ref{word-0} or \ref{word-1} to represent an ``empty'' spot, we cannot
consistently model this reality. Indeed, we have no guarantee that a particular
word is initially either one value or the other. For such purposes, data
structures have \key{constructors}.

\begin{definition}

A constructor is a modification method that resets a data structure to some
particular initial value.

\end{definition}

If we have another instruction, $\proc{Nil}$, which resets all the bits of a
given word to $0$, and let \ref{word-0} represent an empty spot, we are able to
consistently model the real world occupation of the parking lot.

% A program is stored in memory, and therefore a particular word in memory can
% always be initialized to some particular value.

Using a 32-bit word for a boolean is not particularly efficient. It can
represent $2^{32}$ different values and we're only utilizing two
representations\footnotemark!  Another use-case for words is to represent
natural numbers in the range $[0;2^{32})$.  Useful modification methods would
thus be addition, subtraction, multiplication, etc.

\footnotetext{Sometimes it proves practical to waste a little space in favor of
much faster execution time, so while we could do with a single bit to represent
a boolean value, computers often use 1, 2, or even 4 bytes due to the hardware
design.}

As a matter of fact, the common computer comes with these and a myriad of other
instructions, which hence make up the computer's \key{instruction set}. The
``word'' in this context becomes a rather ubiquitous data structure. Most of
the time we won't be needing this ubiquity, and may indeed choose only a small
subset of the instruction set to apply to any particular word in memory.
Indeed, we may desire for a particular word to represent a boolean or a natural
number, but not both at the same time!

While computers could probably support a few more basic data types, and many
do, it has proven more practical to facilitate this sort of ``safety'' at a
different level of abstraction. Enter \key{data types}.

\begin{definition}

A data structure defines a data type. A value of a given data type may only be
modified using a method defined for the corresponding data structure.

\end{definition}

Imperative programming languages facilitate the definition of data types, and
much like computers have basic data structures, they have basic data types. The
actual set of basic data types available to the programmer depends on the
intentions of the programming language, as well as the architecture of the
computer in question. For example, we may be able to modify a single byte of a
word, or modify multiple contiguous words with a single instruction.
\referToTable{basic-types} presents a sample list of basic data types in an
imperative programming language.

\makeTable
{basic-types}
{A sample list of basic data types in an imperative programming language.}
{lcl}
{{\bf Name} & {\bf Bytes} & {\bf Represents}}
{
  \code{byte} & $1$ & A raw byte.\\
  \code{boolean} & $1$ & A boolean.\\
  \code{char} & $2$ & A character.\\
  \code{int} & $4$ & An integer in the range $[-2^{31};2^{31})$.\\
  \code{unsigned int} & $4$ & An integer in the range $[0;2^{32})$.\\
  \code{long} & $8$ & An integer in the range $[-2^{63};2^{63})$.\\
  \code{unsigned long} & $8$ & An integer in the range $[0;2^{64})$.
}

This is not a complete list, and for some particular language, on some
particular architecture the list may look somewhat different, both in terms of
which data types there are, and their sizes.

\begin{definition}

We refer to the set of numbers $\ldots,-2,-1,0,1,2,\ldots$ as \key{integers}.

\end{definition}

\begin{definition}

We refer to a data type that can represent a subset of the integers as an
\key{integer data type}.

\end{definition}

There are a few peculiarities about \referToTable{basic-types}.

Firstly, some data types have an ``unsigned'' counterpart. The difference
between these and their ``signed'' brotheren is easy to see if we consider
their respective value ranges. Integer data types, such as are usually signed
to begin with. In cases where negative values don't make much sense, they can
be explicitly made unsigned.

Second, there are the ranges. An \code{unsigned int} has the range
$[0;2^{32})$, which is to say it can represent the integers
$0,1,\ldots,2^{32}-2,2^{32}-1$. This is pretty intuitive since a sequence of
$4$ bytes can represent $2^{32}$ different values, and we're looking to
represent positive integral values. What's perhaps less intuitive is the range
of an \code{int}, $[-2^{31};2^{31})$. This has to do with the data type being
``signed''. Indeed, a single bit is reserved for repesenting the sign, while
the remaing $31$ are left to represent some absolute value.

This business of signed integers introduces another peculiarity. Consider some
\code{int} value $x=-2^{31}$. What is $-1\cdot x$? For regular integers we know
that the answer is $2^{31}$, but we can't represent this number with an
\code{int}\ldots

Signed integer data types are often \key{asymmetric} in this manner, and this
can be a great source of confusion. The convention is that $x$ is its own
negation, so $-1\cdot x = -2^{31} = x$. Likewise, given $x=2^{31}-1$, $x+1$=$-2^{31}$

\begin{definition}

\key{Integer overflow} occurs whenever the resulting value of a particular
operation is outside the range of values representable by the integer data type
of the result.

\end{definition}

Integer overflow is a case where the origins of digital computers shines
through. Computers are primarily tools for numerical analysis, i.e.  the study
of algorithms for numerical approximation, not general symbolic manipulation.
Disregard of these origins can lead to small mathematical absurdities that very
quickly lead to a thunderstorm on the other side of the globe.

\begin{definition}

Let $[Min;Max]$ denote the range of values representable by a particular
integer data type. Let $|Min|>|Max|$, and $b\in\set{1,2,\ldots,Max}$, if the
data type has integer overflow, then:

\begin{align}
a+b=\left\{
\begin{array}{ll}
Min+b-1&\text{\key{if}}\ a=Max\\
a+b&\text{\key{otherwise}}
\end{array}\right.\\
a-b=\left\{
\begin{array}{ll}
Max-b+1&\text{\key{if}}\ a=Min\\
a-b&\text{\key{otherwise}}
\end{array}\right.\\
\end{align}

\end{definition}

We can WLOG let all other operations on integers be a sequence of additions or
subtractions. Also, this definition duely relates to both signed and unsigned
values.

Because maintaining a data structure generally requires a sequence of actions..
blah.

% A programming language facilitates a programming paradigm if it is both easy,
% and reasonably safe to use the paradigm. Having the ability to define data
% structures, and actions that modify them, is one thing.  Another is having
% assurance that the wrong action is never used to modify the wrong data
% structure. This is where \key{type systems} come in handy.

% We say that a set of data structures defining the same ordering on the same
% \key{types} of data elements, and having the same methods of modification,
% belong to the same \key{class} of data structures. A class of data structures
% is a type of data element, or simply, type. This notion is intentionally
% recursive.

% We hence say that actions are defined on particular types, and a type system
% can hence ensure that the wrong action is not used to modify the wrong data
% structure before the program is ever run. You should by now have plenty of
% experience with the notorious SML type system.

% It is clear that designing a new piece of hardware for every type of action is
% impractical. The desired net effect can often be achieved through a series of
% basic actions. For instance, the sum of $n$ different numbers can be computed
% using $n-1$ binary additions. Hence, we can make due with a \key{sequencing
% mechanism} and good old binary addition, rather than design a new piece of
% hardware for every conceivable $n$.

% A computer, therefore, has a finite set of basic actions, called its
% \key{instruction set}. A computer program is specified in terms of a sequence
% of these basic actions, called a \key{program text}. The computer executes the
% program by performing the sequence of actions in order.

% The sequence of basic actions is not a sequencing mechanism in and of itself.
% Indeed, we are unable to specify a computer program for summing a sequence of
% an arbitrary size $n$, since no sequence of binary additions will ever be
% enough for every conceivable $n$.  What's more, why duplicate the actions?
% Afterall, as we sequentially sum a sequence of numbers we're doing the same
% action, a binary addition, just on different data.

% To mitigate this lack of flexibility, the von Neuman architecture introduces
% the \key{jump} action. Since the program is specified as a finite sequence
% of basic actions, we can easily assign a unique integer index to every action in the sequence.
% As the computer is performing an action at a particular index at any point in time

% The
% idea is then to let the computer have an action that allows to specify an arbitrary action as
% the next action to perform, rather then the one then immediately next
% instruction.

% At this point it makes sense to reconsider the concept of time. The convention is to let all basic
% actions be upper bounded by some constant value. This allows us to analyze the
% running time of computer programs in terms of how much sequencing is going on.


% At any point in time, the computer is performing some particular action, 

% and provide a
% mechanism for performing a sequence of actions in order. This however, still does not allow us to write a 

% Depending on our level of interest, we may consider an action as either an
% atomic happening, or as a sequence of subactions necessary to achieve the
% overall net effect. The subactions themselves may be considered in terms of
% their subsubactions, and so on. Indeed, one possible program design method is
% to start with a high-level overview and to gradually dig into the details.
% Naturally, such a hierarchical ordering of a program also facilitates the human
% comprehension of the program. This calls to attention a quote by Donald
% Knuth\cite{knuth-review-of-sp}:

% \begin{quote}"You have perhaps had a dream much like mine: Wouldn't it be nice
% to have a glorious system of complete maps of the world, whereby one could (by
% turning dials) increase or decrease the scale at will? A similar thing can be
% achieved for programs, rather easily, when we give the programs a hierarchic
% structure like those constructed step-wise. It is not hard to imagine a
% computing system which displays a program in such a way that, by pressing
% an-appropriate button, one can replace the name of a routine by its expansion
% on the next level, or conversely."\end{quote}

% Here the word ``routine'' stands for an action, and ``its expansion on the next
% level'' is the sequence of sub-actions necessary to achive the intended
% net effect. The choice of the word ``routine'' is not an unlucky one. Afterall,
% we all have some sort of routine for doing the laundry, and doing the laundry
% is a rather routine task! Let us now consider the immediate expansion of doing
% the laundry, assuming that we have a common washing machine at our disposal:

The dirty pile of laundry is not a particularly interesting data structure in
and of itself, but it will prove interesting to consider how we morph the data
structure into other well-kept data structures as we do the laundry.

Depending on our level of interest, we may consider an action as an atomic
happening or as a sequence of subactions necessary to achieve the overall net
effect. Each subaction can in turn be considered in terms of its subsubactions,
and so on. Let us consider the possible subactions of doing the laundry,
assuming that we have a common washing machine at our disposal:

\begin{codebox}
\li Split the pile into colors and whites.
\li Wash colors.
\li Wash whites.
\li Make a neat stack of the clean laundry.
\end{codebox}

This is an example of an \key{algorithm}. An algorithm is a sequence of actions
that if performed in order achieves some intended net effect. Formally
speaking, an algorithm has some sort of input, which it transforms into some
sort of output through a sequence of actions. Here, the input is a pile of
dirty laundry, and the output is a neat stack of clean laundry. When specifying
an algorithm we tend to give it a name, and be somewhat more explicit about its
inputs and outputs, for instance:

\begin{codebox}
\Procname{$\proc{Do-The-Laundry}\p{\text{A dirty pile of laundry}}$}
\li Split the pile into colors and whites.
\li Wash colors.
\li Wash whites.
\li Make a neat stack of the clean laundry.
\li \Return the neat stack of clean laundry.
\end{codebox}

We all know that whites and colors don't mix well in a washing machine. Indeed,
a higher temperature should be used for whites, since they are generally harder
to get clean, and a lower temperature should be used for colors, since high
temperatures may leed to color bleeding.

A common mistake is to oversee a colored piece in a pile of whites, and wash
the pile with high temperature.  Most likely, this will lead to all whites in
that pile bleeding from white to a light shade of the mixed-in color. That
would definitely cause a hurricane on the other side of the globe!  As such,
this is a \key{pre-} and \key{postcondition} pair we forgot to mention. Not
only should the pile be clean, but every piece of laundry should also keep its
original color.

\begin{definition}

Assuming that preconditions hold before the action commences, the
postconditions should hold after the action terminates\footnotemark.

\footnotetext{We say ``should'', becuase there is currently no automatic way of
assuring this for an arbitrary action in by far the most practical programming
languages. Hence, it is up to the programmer to manually prove that the
postconditions will hold.}

\end{definition}

We now state the more accurate pre- and postconditions of doing the laundry:

\begin{codebox}
\zi
$\begin{array}{ll}
&\text{Given a dirty pile, the pile is cleaned.}\\
\wedge&\text{Every piece of laundry keeps its original color.}
\end{array}$
\end{codebox}

Here, the symbol $\wedge$ means ``and'', i.e. both statements should hold.

When \key{declaring} an action, we also specify its name, as well input and
output parameters. So a complete declaration of doing the laundry looks like
this:

\begin{codebox}
\Procname{$\proc{Do-The-Laundry}::(Laundry)\rightarrow Laundry$}
\zi
$\begin{array}{ll}
&\text{Given a dirty pile, a clean pile is returned.}\\
\wedge&\text{Every piece of laundry keeps its original color.}
\end{array}$
\end{codebox}



as well as the pre- and postconditions. When \key{defining}
an action, we specify its actual sequence of subactions. We make this
distinction because an action declaration may well have several definitions.

Of course, at some point, some real work has to get done.

A von-Neuman computer has a basic set of actions that it can perform.


The initial pile of dirty laundry is not a particularly interesting data
structure, but it will prove beneficial to morph this data structure into other
types as we \emph{progress} through the action. Indeed, 

Of course, at some point we must reach basic, non-divisible 


 i.e.  doing
the laundry, or as a sequence of subactions. This sequence represents the steps
necessary to achieve the overall intended net effect.


(Naur,
function vs. process).

\begin{codebox}
\Procname{$\proc{Do-The-Laundry}\p{dirtyPile}$}
\li \Return $cleanPile$
\end{codebox}

We can dissect this definition into
\key{pre-conditions} and \key{post-conditions}, and state their relationship in
terms of an \key{implication}, denoted by the symbol $\Rightarrow$:

\begin{equation}
\text{Dirty pile}\Rightarrow \text{Clean pile}.
\end{equation}



More formally, if at time $T_n$ we
have a pile of dirty laundry, and perform an action in the subsequent time slot
$T_1$, then at time $T_{n+1}$ we have a pile of clean laundry.


To define the
net effect of an action, we state the conditions that must hold at time
$T_{n+1}$, given that certain conditions hold at time $T_n$. We call these
conditions the \key{post-} and \key{pre-conditions}, respectively. Given an
action $a$, we say that if the preconditions hold, then after the action, the
post-conditions must hold, denoted

\begin{equation}
\text{Precondition} \Rightarrow^a \text{Postcondition}.
\end{equation}


The effect
of the action is that a precondition \emph{implies} a post condition, we denote
this with the symbol $\Rightarrow$.

It does not make sense to do the laundry without a pile of dirty laundry, and a
good ``doing the laundry'' action leaves all the laundry clean after the action
took place. We therefore state the following:

 Before the action
commences, you have a pile of dirty laundry; after the action, the laundry is
clean. We call these the \key{pre-} and \key{postconditions} of an action,
respectively. In particular, it does not make sense to do the laundry unless
you have a pile of dirty laundry, and the post-condition that the laundry is
clean is probably useful for future actions.

The post-conditions are not complete. There are certain things that we would
like to do to facilitate further actions. For instance, it is useful if in the
process of doing the laundry we didn't mix clean and dirty laundry, and at best
-- neatly stacked the clean laundry in the closet.


The post-conditions are not
completely well-formed however. In particular, 

where whites are mixed with
colors. Anyone who's ever done their laundry knows that it's no good to mix 

After the action, the clothes should be neatly stacked in the closet. 

% doing the laundry 
